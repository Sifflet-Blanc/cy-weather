# ğŸ“š Scripts MLflow pour le Cours DevMLOps

Ce dossier contient une sÃ©rie de scripts pÃ©dagogiques pour apprendre Ã  utiliser MLflow dans un contexte de DevMLOps.

## ğŸ¯ Objectifs du cours

Apprendre aux Ã©tudiants Ã :
- âœ… Monitorer leurs modÃ¨les de Machine Learning
- âœ… Enregistrer et versionner leurs modÃ¨les
- âœ… TÃ©lÃ©charger automatiquement la derniÃ¨re version d'un modÃ¨le
- âœ… DÃ©tecter le data drift
- âœ… Comparer les performances de diffÃ©rents modÃ¨les

## ğŸ“‹ Scripts disponibles

### 1ï¸âƒ£ **mlflow_01_basic_logging.py** - Enregistrement basique
**Concepts abordÃ©s:**
- Logger des paramÃ¨tres (hyperparamÃ¨tres)
- Logger des mÃ©triques (accuracy, f1-score, etc.)
- Logger un modÃ¨le entraÃ®nÃ©
- Logger des artefacts (graphiques, fichiers)
- Utiliser des tags pour organiser les runs

**Utilisation:**
```bash
python mlflow_01_basic_logging.py
```

---

### 2ï¸âƒ£ **mlflow_02_model_registry.py** - Model Registry & Versioning
**Concepts abordÃ©s:**
- Enregistrer un modÃ¨le dans le Model Registry
- GÃ©rer les versions de modÃ¨les
- Changer les stages (None â†’ Staging â†’ Production â†’ Archived)
- Ajouter des descriptions et tags aux versions
- Comparer automatiquement les performances entre versions

**Utilisation:**
```bash
python mlflow_02_model_registry.py
```

---

### 3ï¸âƒ£ **mlflow_03_load_latest_model.py** - Chargement de modÃ¨les
**Concepts abordÃ©s:**
- Charger un modÃ¨le par stage (Production, Staging)
- Charger une version spÃ©cifique
- Charger la derniÃ¨re version entraÃ®nÃ©e
- Charger depuis un Run ID
- Utiliser le modÃ¨le pour faire des prÃ©dictions

**Utilisation:**
```bash
# D'abord, exÃ©cutez le script 2 pour crÃ©er des modÃ¨les
python mlflow_02_model_registry.py

# Puis chargez le modÃ¨le
python mlflow_03_load_latest_model.py
```

---

### 4ï¸âƒ£ **mlflow_04_parameter_tuning.py** - Monitoring & Hyperparameter Tuning
**Concepts abordÃ©s:**
- EntraÃ®ner plusieurs modÃ¨les avec diffÃ©rents hyperparamÃ¨tres
- Logger et comparer les performances
- Utiliser des runs parents/enfants pour organiser les expÃ©rimentations
- Trouver automatiquement le meilleur modÃ¨le
- Utiliser l'API de recherche MLflow

**Utilisation:**
```bash
python mlflow_04_parameter_tuning.py
```

**Note:** Ce script peut prendre quelques minutes (teste 48 combinaisons d'hyperparamÃ¨tres).

---

### 5ï¸âƒ£ **mlflow_05_data_drift_detection.py** - DÃ©tection de Data Drift
**Concepts abordÃ©s:**
- DÃ©tecter les changements dans la distribution des donnÃ©es
- Calculer le Population Stability Index (PSI)
- Utiliser le test de Kolmogorov-Smirnov
- Comparer les statistiques entre train et production
- CrÃ©er des visualisations de drift
- Alerter en cas de drift significatif

**Utilisation:**
```bash
python mlflow_05_data_drift_detection.py
```

---

## ğŸš€ Ordre d'exÃ©cution recommandÃ©

Pour une expÃ©rience d'apprentissage optimale:

1. **DÃ©marrez avec le script 1** pour comprendre les bases du logging
2. **Passez au script 2** pour voir comment gÃ©rer les versions
3. **ExÃ©cutez le script 3** pour apprendre Ã  charger des modÃ¨les
4. **Testez le script 4** pour comparer des modÃ¨les
5. **Finissez avec le script 5** pour la dÃ©tection de drift

## ğŸ“Š Visualisation dans MLflow UI

AprÃ¨s avoir exÃ©cutÃ© les scripts, consultez l'interface MLflow:

```
http://localhost:5000
```

### Navigation dans l'UI:

- **Experiments** ğŸ“‚ : Voir tous vos runs organisÃ©s par expÃ©rience
- **Models** ğŸ·ï¸ : Consulter le registry et les versions de modÃ¨les
- **Compare** âš–ï¸ : Comparer plusieurs runs cÃ´te Ã  cÃ´te
- **Charts** ğŸ“ˆ : Visualiser les mÃ©triques sous forme de graphiques

## ğŸ’¡ Bonnes pratiques enseignÃ©es

### 1. Organisation des expÃ©riences
```python
mlflow.set_experiment("nom_descriptif")
```

### 2. Nommage des runs
```python
with mlflow.start_run(run_name="description_claire"):
```

### 3. Logging structurÃ©
```python
# ParamÃ¨tres: configuration du modÃ¨le
mlflow.log_params({"n_estimators": 100, "max_depth": 5})

# MÃ©triques: rÃ©sultats quantitatifs
mlflow.log_metrics({"accuracy": 0.95, "f1_score": 0.93})

# Tags: mÃ©tadonnÃ©es qualitatives
mlflow.set_tags({"team": "data-science", "environment": "prod"})
```

### 4. Gestion des versions en production
```python
# En production, toujours utiliser le stage
model_uri = "models:/model_name/Production"
model = mlflow.sklearn.load_model(model_uri)
```

### 5. Monitoring continu
- Logger les performances en production
- DÃ©tecter le data drift rÃ©guliÃ¨rement
- Automatiser le rÃ©-entraÃ®nement si drift > seuil

## ğŸ› ï¸ Configuration

Tous les scripts utilisent le mÃªme serveur MLflow:
```python
mlflow.set_tracking_uri("http://localhost:5000")
```

Pour utiliser un serveur diffÃ©rent, modifiez cette ligne dans chaque script.

## ğŸ“¦ DÃ©pendances

```bash
pip install mlflow scikit-learn matplotlib numpy scipy
```

## ğŸ“ Exercices pour les Ã©tudiants

### Exercice 1: Modifier les paramÃ¨tres
Modifiez `mlflow_04_parameter_tuning.py` pour tester d'autres algorithmes (SVM, KNN, etc.).

### Exercice 2: Ajouter des mÃ©triques
Dans `mlflow_01_basic_logging.py`, ajoutez le logging de nouvelles mÃ©triques (recall, ROC-AUC, etc.).

### Exercice 3: Monitoring en temps rÃ©el
CrÃ©ez un script qui charge le modÃ¨le en production et surveille ses performances en continu.

### Exercice 4: Alertes automatiques
Modifiez `mlflow_05_data_drift_detection.py` pour envoyer une alerte (email, Slack) en cas de drift.

### Exercice 5: CI/CD Pipeline
IntÃ©grez ces scripts dans un pipeline CI/CD (GitHub Actions, GitLab CI, etc.).

## ğŸ”— Ressources additionnelles

- [Documentation MLflow](https://mlflow.org/docs/latest/index.html)
- [MLflow Model Registry](https://mlflow.org/docs/latest/model-registry.html)
- [Evidently AI (drift detection)](https://www.evidentlyai.com/)
- [Great Expectations](https://greatexpectations.io/)

## ğŸ“ Notes pÃ©dagogiques

### Points clÃ©s Ã  souligner:

1. **ReproductibilitÃ©**: MLflow garantit que les expÃ©riences sont reproductibles
2. **TraÃ§abilitÃ©**: Chaque run est enregistrÃ© avec tous ses paramÃ¨tres
3. **Collaboration**: L'Ã©quipe peut voir et comparer les rÃ©sultats
4. **Production-ready**: Le Model Registry facilite le dÃ©ploiement
5. **Gouvernance**: Les stages permettent de contrÃ´ler ce qui est dÃ©ployÃ©

### Erreurs communes Ã  Ã©viter:

âŒ Ne pas logger les paramÃ¨tres â†’ impossible de reproduire
âŒ Ne pas versionner les modÃ¨les â†’ confusion sur ce qui est dÃ©ployÃ©
âŒ Ignorer le data drift â†’ dÃ©gradation silencieuse des performances
âŒ Ne pas tester avant de promouvoir en Production
âŒ Oublier de documenter les modÃ¨les (descriptions, tags)

## ğŸ¤ Support

Pour toute question sur les scripts ou MLflow:
- Consultez la documentation MLflow
- Posez des questions pendant le cours
- ExpÃ©rimentez et testez diffÃ©rentes configurations!

---

**Bon apprentissage! ğŸš€ğŸ“ŠğŸ¤–**
